{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad32605",
   "metadata": {},
   "source": [
    "# RAG Pipeline Exercise\n",
    "\n",
    "In this exercise you will build and **compare two simple Retrieval-Augmented Generation (RAG) pipelines**.\n",
    "\n",
    "You will work with a small collection of PDF documents (e.g. medical guidelines) and:\n",
    "\n",
    "1. Load and chunk the PDF documents.\n",
    "2. Create a vector index using **embedding model A** (local `BAAI/bge-m3`).\n",
    "3. Create a second index using **embedding model B** (e.g. OpenAI or Gemini embeddings).\n",
    "4. Implement a simple **retriever** and an **answering function** that calls an LLM with retrieved context.\n",
    "5. Automatically **generate questions** from the documents and use them to **compare two RAG configurations**.\n",
    "\n",
    "Cells marked with `# TODO` are **for students to implement**.\n",
    "Everything else is provided scaffolding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf82e6",
   "metadata": {},
   "source": [
    "## 0. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be93ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (easy): skim the imports and make sure you understand what each library is used for.\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# LLM / API clients (we will mainly use OpenAI here; Gemini can be added as a bonus)\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f16980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys from .env (you need to create this file once and add your keys)\n",
    "load_dotenv()\n",
    "\n",
    "deepinfra_key = os.getenv(\"DEEPINFRA_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba2719d",
   "metadata": {},
   "source": [
    "## 1. Load PDF documents\n",
    "\n",
    "We assume there is a `data/` folder containing one or more PDF files.\n",
    "\n",
    "**Task:** implement `load_pdfs(glob_path)` so that it:\n",
    "- Iterates over all PDF files matching `glob_path`\n",
    "- Reads them with `PdfReader`\n",
    "- Concatenates the text of all pages into **one long string**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8abcb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs(glob_path: str = \"data/*.pdf\") -> str:\n",
    "    \"\"\"Load all PDFs matching the pattern and return their combined text.\n",
    "\n",
    "    TODO:\n",
    "    - Use `glob.glob(glob_path)` to iterate over file paths\n",
    "    - For each file, open it in binary mode and create a `PdfReader`\n",
    "    - Loop over `reader.pages` and extract text with the extract_text() function\n",
    "    - Concatenate everything into a single string `text`\n",
    "    - Be robust: skip pages where `extract_text()` returns None\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    text = \"\"\n",
    "    for pdf_path in glob.glob(glob_path):\n",
    "        \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416c3ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once and inspect\n",
    "raw_text = load_pdfs(\"data/*.pdf\")\n",
    "print(\"Number of characters:\", len(raw_text))\n",
    "print(\"Preview:\", raw_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0c5a14",
   "metadata": {},
   "source": [
    "## 2. Chunk the text\n",
    "\n",
    "We will split the long text into overlapping chunks.\n",
    "\n",
    "Later you can **experiment** with different `chunk_size` and `chunk_overlap` to see how it affects retrieval.\n",
    "\n",
    "**Task:** start with the given parameters, run once, then try at least one alternative configuration and note the effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e3f802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base configuration (RAG A)\n",
    "chunk_size_a = 2000\n",
    "chunk_overlap_a = 200\n",
    "\n",
    "splitter_a = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size_a,\n",
    "    chunk_overlap=chunk_overlap_a\n",
    ")\n",
    "\n",
    "chunks_a = splitter_a.split_text(raw_text)\n",
    "print(f\"RAG A: {len(chunks_a)} chunks produced, first chunk length = {len(chunks_a[0])}\")\n",
    "\n",
    "# TODO (mini-experiment): change chunk_size / chunk_overlap for RAG B and compare\n",
    "chunk_size_b =    # e.g. smaller chunks\n",
    "chunk_overlap_b = \n",
    "\n",
    "splitter_b = \n",
    "\n",
    "chunks_b = splitter_b.split_text(raw_text)\n",
    "print(f\"RAG B: {len(chunks_b)} chunks produced, first chunk length = {len(chunks_b[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3511f4",
   "metadata": {},
   "source": [
    "## 3. Create embeddings and a FAISS index\n",
    "\n",
    "We start with **Embedding model A: `BAAI/bge-small-en`** using `sentence-transformers`. You can find a list of more models here: https://huggingface.co/spaces/mteb/leaderboard \n",
    "make sure that the models are not bigger than the one used here. Otherwise the embeddings process will take too long.\n",
    "\n",
    "Then, as an optional extension, you can build **Embedding model B** using OpenAI or Gemini and compare.\n",
    "\n",
    "To keep the exercise manageable, the base version only **requires** BGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b519161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model A (local)\n",
    "model_name_a = \"BAAI/bge-small-en\"\n",
    "embedder_a = SentenceTransformer(model_name_a)\n",
    "\n",
    "# Compute embeddings for all chunks of configuration A\n",
    "embeddings_a = embedder_a.encode(chunks_a, convert_to_numpy=True)\n",
    "\n",
    "dimensions_a = embeddings_a.shape[1]\n",
    "print(\"Embedding dimensionality (A):\", dimensions_a)\n",
    "\n",
    "index_a = faiss.IndexFlatL2(dimensions_a)\n",
    "index_a.add(embeddings_a)\n",
    "print(\"FAISS index (A) size:\", index_a.ntotal)\n",
    "\n",
    "# Persist index/chunks if you like (optional)\n",
    "os.makedirs(\"faiss\", exist_ok=True)\n",
    "faiss.write_index(index_a, \"faiss/faiss_index_a.index\")\n",
    "with open(\"faiss/chunks_a.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks_a, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77271916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model B using OpenAI embeddings.\n",
    "\n",
    "# TODO :\n",
    "# - Use `openai_client.embeddings.create(...)` to compute embeddings for `chunks_b`\n",
    "# - Create a second FAISS index `index_b`\n",
    "# - Make sure to check the dimensionality from the first embedding vector\n",
    "\n",
    "\n",
    "openai_client = OpenAI(api_key=openai_api_key)\n",
    "response = openai_client.embeddings.create(\n",
    "     model=\"text-embedding-3-small\",\n",
    "    input=chunks_b\n",
    ")\n",
    "embeddings_b = np.array([item.embedding for item in response.data])\n",
    "dim_b = \n",
    "index_b = \n",
    "index_b.\n",
    "print(\"FAISS index (B) size:\", index_b.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0339224e",
   "metadata": {},
   "source": [
    "## 4. Implement a simple retriever\n",
    "\n",
    "We now implement a generic retrieval function that:\n",
    "1. Embeds the query.\n",
    "2. Searches the FAISS index.\n",
    "3. Returns the corresponding text chunks.\n",
    "\n",
    "We implement it for configuration A. If you built configuration B, you can reuse the same function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6fbf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_texts(query: str, k: int, index, chunks, embedder) -> list:\n",
    "    \"\"\"Return the top-k most similar chunks for a query.\n",
    "    - Encode the query with `embedder.encode(...)`\n",
    "    - Call `index.search(query_embedding, k)`\n",
    "    - Use the returned indices to select the chunks\n",
    "    - Return a list of strings (chunks)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    query_emb = embedder.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_emb, k)\n",
    "    retrieved = [chunks[i] for i in indices[0]]\n",
    "    return retrieved\n",
    "\n",
    "# Quick sanity check\n",
    "test_query = \"What is the most important factor in diagnosing asthma?\"\n",
    "retrieved_test = \n",
    "print(\"Number of retrieved chunks:\",)\n",
    "print(\"Preview of first chunk:\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f646fdc6",
   "metadata": {},
   "source": [
    "## 5. Implement `answer_query` using an LLM\n",
    "\n",
    "Now we build the actual RAG call:\n",
    "\n",
    "1. Use `retrieve_texts` to get top-`k` chunks.\n",
    "2. Concatenate them into a context string.\n",
    "3. Build a prompt that:\n",
    "   - shows the context\n",
    "   - asks the model to answer the user question based **only** on this context.\n",
    "4. Call the OpenAI chat completion API.\n",
    "\n",
    "This is the **core RAG function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d94610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query: str, k: int, index, chunks, embedder, client: OpenAI) -> str:\n",
    "    \"\"\"RAG-style answer: retrieve context and ask an LLM.\n",
    "\n",
    "    TODO (students):\n",
    "    - Use `retrieve_texts` to get `k` relevant chunks.\n",
    "    - Join them into a single context string.\n",
    "    - Build a chat prompt that instructs the model to answer *only* using the context.\n",
    "    - Call `client.chat.completions.create(...)` with model `\"gpt-5o-mini\"` (or similar).\n",
    "    - Return the model's answer text.\n",
    "    \"\"\"\n",
    "    retrieved_chunks = retrieve_texts(query, k, index, chunks, embedder)\n",
    "    context = \n",
    "\n",
    "    # Remember: strings can be concatenated (like an addition)\n",
    "    system_prompt = (\n",
    "        \"add prompt with context\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "    ]\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-5o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content.strip()\n",
    "\n",
    "# Quick manual test\n",
    "answer = answer_query(test_query, k=3, index=index_a, chunks=chunks_a, embedder=embedder_a, client=openai_client)\n",
    "print(\"RAG answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a86a92e",
   "metadata": {},
   "source": [
    "## 6. Generate questions from random chunks (automatic evaluation set)\n",
    "\n",
    "To compare two RAG configurations, we need **questions**.\n",
    "\n",
    "We will:\n",
    "- randomly sample a few chunks from the corpus,\n",
    "- ask an LLM to generate a **good question** whose answer is contained in the chunk.\n",
    "\n",
    "Then we can use these questionâ€“chunk pairs as a small evaluation set.\n",
    "\n",
    "We provide most of the implementation. Your job is mainly to:\n",
    "- inspect the code,\n",
    "- understand the prompt,\n",
    "- maybe tweak the number of chunks or retries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb899fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions_for_random_chunks(chunks, num_chunks: int = 5, max_retries: int = 2):\n",
    "    selected_chunks = random.sample(chunks, num_chunks)\n",
    "    qa_pairs = []\n",
    "\n",
    "    for chunk in selected_chunks:\n",
    "        prompt = prompt = (\n",
    "            \"Based on the following text, generate an insightful question that covers its key content:\\n\\n\"\n",
    "            \"Text:\\n\" + chunk + \"\\n\\n\"\n",
    "            \"Question:\"\n",
    "        )\n",
    "\n",
    "        question = None\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                completion = openai_client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )\n",
    "                question = completion.choices[0].message.content.strip()\n",
    "                if question:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(\"Error while generating question, retrying...\", e)\n",
    "\n",
    "        if question is None:\n",
    "            question = \"Error: could not generate question.\"\n",
    "\n",
    "        qa_pairs.append((chunk, question))\n",
    "\n",
    "    return qa_pairs\n",
    "\n",
    "questions = generate_questions_for_random_chunks(chunks_a, num_chunks=5, max_retries=2)\n",
    "for i, (chunk, q) in enumerate(questions, 1):\n",
    "    print(f\"Q{i}: {q}\\n  From chunk preview: {chunk[:120]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd0eaf5",
   "metadata": {},
   "source": [
    "## 7. Compare two RAG configurations\n",
    "\n",
    "Now we can:\n",
    "- Use the generated questions,\n",
    "- Answer them with RAG configuration A (BGE + chunking A),\n",
    "- (Optional) Answer them with RAG configuration B (e.g. different chunking and/or different embeddings),\n",
    "- Compare the answers qualitatively.\n",
    "\n",
    "To keep the exercise manageable, we start with config A only.\n",
    "If you implemented config B, reuse `answer_query` with `index_b`, `chunks_b`, and your second embedder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a292474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_generated_questions(question_tuples, k, index, chunks, embedder, client):\n",
    "    results = []\n",
    "    for chunk, question in question_tuples:\n",
    "        answer = answer_query(question, k, index, chunks, embedder, client)\n",
    "        results.append({\n",
    "            \"chunk\": chunk,\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "    return results\n",
    "\n",
    "results_a = answer_generated_questions(\n",
    "    questions,\n",
    "    k=5,\n",
    "    index=index_a,\n",
    "    chunks=chunks_a,\n",
    "    embedder=embedder_a,\n",
    "    client=openai_client,\n",
    ")\n",
    "\n",
    "for item in results_a:\n",
    "    print(\"Question:\", item[\"question\"])\n",
    "    print(\"Answer A:\", item[\"answer\"])\n",
    "    print(\"Source chunk preview:\", item[\"chunk\"][:150], \"...\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec212b7",
   "metadata": {},
   "source": [
    "Add RAG B and create a comparison table\n",
    "\n",
    "If you implemented a second configuration (e.g. different chunking + OpenAI embeddings):\n",
    "\n",
    "1. Build `index_b` and `embedder_b`.\n",
    "2. Run `results_b = answer_generated_questions(..., index_b, chunks_b, embedder_b, client)`.\n",
    "3. For each question, compare:\n",
    "   - Which answer is more complete / specific?\n",
    "   - Which one is better grounded in the source chunk?\n",
    "4. Summarise your findings in a short **markdown cell** or a small table.\n",
    "\n",
    "---\n",
    "\n",
    "This concludes the core RAG exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cda81a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
